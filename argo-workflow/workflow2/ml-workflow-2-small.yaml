apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  name: llm-train
spec:
  entrypoint: main-dag
  arguments:
    parameters:
      - name: model-name
        value: sshleifer/tiny-gpt2
      - name: run-train
        value: "false"  
  volumes:
    - name: shared-data
      persistentVolumeClaim:
        claimName: model-pvc
  templates:
    - name: main-dag
      dag:
        tasks:
          - name: train-and-predict-path
            template: dag-train-and-predict
            when: "{{workflow.parameters.run-train}} == true"
            arguments:
              parameters:
                - name: model-name
                  value: "{{workflow.parameters.model-name}}"

          - name: predict-only-path
            template: dag-predict-only
            when: "{{workflow.parameters.run-train}} == false"
            arguments:
              parameters:
                - name: model-name
                  value: "{{workflow.parameters.model-name}}"

    - name: dag-train-and-predict
      inputs:
        parameters:
          - name: model-name
      dag:
        tasks:
          - name: train
            template: train-model
            arguments:
              parameters:
                - name: model-name
                  value: "{{inputs.parameters.model-name}}"

          - name: predict
            dependencies: [train]
            template: run-inference

    - name: dag-predict-only
      inputs:
        parameters:
          - name: model-name
      dag:
        tasks:
          - name: predict
            template: run-inference

    - name: train-model
      inputs:
        parameters:
          - name: model-name
      container:
        image: private.registry.com:31445/lightweight-llm-trainer:latest
        command: ["python"]
        args: ["train_model_small.py", "{{inputs.parameters.model-name}}"]
        volumeMounts:
          - name: shared-data
            mountPath: /mnt/output
        resources:
          requests:
            memory: "2Gi"
            cpu: "1000m"
          limits:
            memory: "4Gi"
            cpu: "2000m"
        nodeSelector:
          kubernetes.io/hostname: "ubuntu02"

    - name: run-inference
      container:
        image: private.registry.com:31445/lightweight-llm-trainer:latest
        command: [python]
        args: ["predict_model_small.py"]
        volumeMounts:
          - name: shared-data
            mountPath: /mnt/output
        resources:
          requests:
            memory: "2Gi"
            cpu: "1000m"
          limits:
            memory: "4Gi"
            cpu: "2000m"
        nodeSelector:
          kubernetes.io/hostname: "ubuntu02"
